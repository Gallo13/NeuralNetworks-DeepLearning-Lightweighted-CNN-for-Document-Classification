# NeuralNetworks-DeepLearning-Lightweighted-CNN-for-Document-Classification

*Used Lightweighted CNN for Document Classification by Ritu Yadav (forked)

Full answers to these questions are in repository. No pictures are shown here.

Pick-up your project from the papers and links listed in the Project 2 site. All of them are papers with a code. Pick-up a paper that is published after 2014.

a) Problem Description: What is the problem that you will be investigating? Provide paper titles, authors and other detail and upload the paper on the BB.

The problem I will be investigating is the Light-Weighted CNN for Text Classification by Ritu Yadav. The paper discuses a way to reduce trainable parameters and memory consumption. To do this, they looked into a new architecture that uses some new concept already existing in the image classification world, that can be used for text classification and cause lesser memory consumption. They came up with three solutions: an optimized way of training Text CNN network, a lightweight CNN and a CNN with a dual optimizer.

b) Data: What data will you use? Usually in the papers they present experiments using more than one dataset. Pick-up one that you will be using for your experiments. It could be a particular dataset from a chosen paper or one that you propose. Upload it on the BB and provide details.

Data: What data will you use? Usually in the papers they present experiments using more than one dataset. Pick up one that you will be using for your experiments. It could be a particular dataset from a chosen paper or one that you propose.
I will use the same data that the paper uses, which is the Tabaco3482 dataset. They give a data folder with text files labeled 0-9. I used data files 0.txt, 6.txt and 8.txt.  These files contain text from the Tobacco-3482 dataset. The categories are [‘ADVE’, ‘Email’, ‘Form’, ‘Letter’, ‘Memo’, ‘News’, ‘Note’, ‘Report’, ‘Resume’, ‘Scientific’]. 


c) Methodology/Algorithm: In the papers very often they propose more than one approach. Pick-up the one that you like and provide details of it.

The paper offers three different approaches to achieve a reduced trainable parameters and memory consumption, which are an optimized Text CNN architecture, a lightweight CNN and a dual optimizer CNN. I have chosen the dual optimizer CNN. What this architecture does is take advantage of the two best optimizers and uses them at different stages of training. They used a hybrid simple strategy SWATS, that Switches from Adam to SGD when a triggering condition is satisfied. The Adam optimizer was picked because it helps the network learn fast, but after a few epochs, the learning process needs to slow down to stabilize performance. They used SGD with momentum as the second optimizer for the later stage of train since it performs better on later epochs. From their results, this particular solution they came up with cut the training time from the base TextCNN by and hour and a half. From their results, it only took 26 minutes to train, however the accuracy was not that high, but it was not the lowest. The accuracy was only at 43.5. The number of trainable parameters was greatly reduced from the base Text CNN. These results were from the larger Tabacco3482 dataset. 

d) Your Approach: This section details the framework of your project. Be specific.

Our approach for this project is to change the optimizer. I decided that I would keep Adam for its swift training and the goal is to reduce training time, but I need an optimizer as good as SGD. I chose RMSProp. I chose this because it helps diminish learning rates by using a moving average of the squared gradient. It utilizes the magnitude of the recent gradient descents to normalize the gradient. In RMSProp, learning rate gets adjusted automatically and it chooses a different learning rate for each parameter. It also divides the learning rate by the average of the exponential decay of squared gradients. For the second test, I again kept Adam and changed SGD/RMSProp to Adadelta because it is basically Adam, which performs very well, but it implements momentum, like the original SGD to smooth gradients based on accumulated “first-order” information.
I wanted to run the original file (SWAT_LW_train.py) in Jupyter Notebook and see the results. I tried running it and experienced a lot of problems. Firstly, was that this code uses from tensorflow.contrib import learn, which is deprecated and no longer runnable on Tensorflow 2.0, so I had to change the environment to Tensorflow 1.14 in order for that to work. This was the first time using an older version of Tensorflow, and after some time working with the code, I realized I might have been able to keep our current Tensorflow 2.1 version and just add tf.compat.v1. to all the older lines with contrib, learn and VocabularyProcessor. The next problem was an error message with “unrecognizable flag error: unkown command line flag f”, so I added tf.compat.v1.flags.DEFINE_string(‘f’, ‘’, ‘’) to resolve this error. The next error was UnicodeEncodeError:’charmap’ codec can’t encode character ‘\ufb01’ in position 282: character maps to <undefined>. To resolve this, I had to open the data_helpers.py file. On line 65, I had to change data = list(open(read_file, “r”).readlines()) to data = list(open(read_file, “r”, encoding=’utf-8’).readlines()).  I then added os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' to fix tensorflow printing logging messages about memory allocation. Then I started training the model. The model was ok, but the accuracy was very low. I did not change any of the parameters, so there were 20 epochs, an evaluation after every 100 steps and a checkpoint save at every 100 steps as well. The training process was a bit different since it showed steps instead of epochs. After the first 100 steps trained, it was supposed to give an evaluation and save that checkpoint, but instead, the kernel buffered and died. I tried running it many times to no avail. The issue was with memory allocation and I received the error message: tensorflow allocation of exceeds 10% of system memory killed. To try to fix this, I tried to allocate more memory to Jupyter Notebook by opening jupyter_notebook_config.py and changing NotebookApp.max_buffer_size to 12000000000 bytes, which is 12GB. This lessened the memory allocation amount, but didn’t fix the issue. I also tried changing all float32 datatypes to float16 to use less memory. Neither of these worked. I then started cutting out the data files until I were only left with 0.txt. I also cut the epoch size to 10 and evaluating and saving checkpoints every 50 steps to help it run. The problem with keeping only 0.txt, though, was that it gave us very weird loss amounts and 100% accuracy:

[picture of training]

Many combinations later, I finally found a combination that allowed better training while still being able to run without memory allocation problems. The data I used was 0.txt, 6.txt and 8.txt. These were the smallest files at 141KB, 51KB and 282KB, respectively. I added another folder to the data folder to put all the unused data files in, called cut-data. These are the results with 0.txt, 6.txt and 8.txt:

[picture]

Our models ran a lot better than the original model described in the paper because the data was much smaller. The original file (which is the training shown above) is labeled NNDL_Project2_SWAT_SGD(smalldata). The trainable parameters were much smaller than the original as well. 

[tables]

There were problems using the eval.py given. After fixing all the errors (such as removing compat.v1 where it was not needed and wouldn’t run with it there), it still gave us error messages . I tried running it directly after training with !python eval_SGD_small.py --eval_train --checkpoint_dir="./runs/1607457019/checkpoints/", which is the command it showed us to use in the readme.txt. The error is:

[error messages]

There seems to be a problem with batch normalization. The problem might be related to the WARNING message from training which says:

[picture of warning]

One thing to note is that for this training, I did not get the same warning message about batch normalization. I didn’t fix it when running the first time since it was just a warning about the older syntax I were using was deprecated. I tried running a different evaluation with model = load_model('./runs/1607457019/checkpoints/model-2950.meta', compile=False) that I would normally use, but it can’t be used since the model would have to be in .h5 format, which it is not. 

[error message]

I then tried using the update showed in the Warning. The update was using tk.keras.layers. That only gave us errors since I would have had to change everything to tk.keras and change the format a bit. I tried many other different approaches to fix the issue, but nothing worked.

[table of evaluation]

e) Experiment(s): You use the respective code that was proposed in the chosen paper. Then show in details the results of your experiments. By details, I mean both quantitative evaluations (show numbers, figures, tables, etc.) as well as qualitative results (show images, example results, etc.).

f) Report: How do you evaluate your results? Analyze and compare your results with those proposed in the respective paper (for this report only not less than 1 - 2 full page(s), Times New Roman 11, single space).

The paper used was the Light-Weighted CNN for Text Classification. It optimizes the TextCNN architecture. They gave three different architectures, an optimized TextCNN architecture, a Lightweight CNN and a Dual Optimizer Optimized CNN. I used the Dual Optimizer CNN for my project. The original Dual Optimizer code was located in SWAT_LW_train.py. The original uses an embedding dimension of 200, filter sizes 2, 3 and 5, 128 number of filters, a dropout of 0.5, an L2 regularization lambda of 0.0001, a batch size of 32, 20 epochs, evaluation every 100 steps, checkpoint every 100 steps and 5 checkpoints stored. When running this on my machine with a 16GB CPU, I were unable to run it. The parameters I change were the number of epochs to 10, evaluation at every 50 steps and a checkpoint at every 50 to help with memory allocation issues. With these new parameters, I re-ran the original file and were still unable to train. I then changed the data given to only using data files 0.txt, 6.txt and 8.txt. I chose to use these three instead of just one of the larger data files for a better mixture of data. The data used was from Tobacco-3482, which includes the categories ADVE, Email, Form, Letter, Memo, News, Note, Report, Resume, and Scientific. 
  The Dual Optimizer they trained on the Tobacco-3482 dataset gave 16496856 trainable parameters, an accuracy of 43.5, and a loss of 1.90 with 26-minute training time. The dual optimizer they trained on their Tobacco-small3482 dataset gave 6057603 trainable parameters, and accuracy of 83, a loss of 0.54 and a training time of 2 minutes. When I ran the dual optimizer with the data provided in the data folder from the Tobacco-3482 dataset on my machine, I couldn’t run it. I couldn’t get those results to compare with the full dataset. I were only able to get the first 100 steps, which had a very low accuracy rate during training, an input size of 87847, and 16497192 trainable parameters. For the file I were able to run with my own small dataset and smaller epoch size, I had 3224697 trainable parameters, with an input size of 9549, and at the last evaluation at step 2950, I got a loss of 0.749915, an accuracy of 0.724759 (about 72), and a training time of 34 minutes. From the results, I got about the same training time as their large data, but a closer accuracy as their small data results and my trainable parameters are closer to their small dataset as well. This could be because the Tobacco-small3482 dataset they used only had 3 categories, but more data in general, where my small data had all the categories, but much less data. 
 For my first experiment was with RMSProp. I decided to keep the Adam optimizer used in the original architecture and change SGD to RMSProp. With this optimizer, I got 3224697 trainable parameters, 9549 training input set, by the last evaluation on step 2950, I got an accuracy of 0.708421 (70), a loss of 1.02848 and a training time of 32 minutes. Of course, like in my smaller SGD file, I had a much smaller number of trainable parameters since my dataset is very small. my accuracy was also a lot better than their larger dataset for the same reason. However, I still didn’t get the same and not even better accuracy than their small dataset. They had about a 10% better accuracy score on their small dataset compared to this one with RMSProp. I had a better accuracy of about 35% better accuracy and their larger dataset. The training time, for some reason, took the same amount of time as their large dataset. When comparing my own smaller SGD file with RMSProp, the SGD optimizer performed a bit better with a 72% accuracy compared to the 70% accuracy I got with RMSProp. The loss is very high on this model, which means this model isn’t that great. Loss throughout training, in the beginning steadily decreased, but through steps 251 to 279 loss fluctuated a lot. For the rest of training loss was fairly stably decreasing. For accuracy, throughout training, seemed to increase, but with a lot of fluctuation. It wasn’t as stable. It seems that if I had a graph, it would have looked like the accuracy graph from the large Tobacco-3482 dataset model from A4: accuracy graph of Lightweight Text CNN with Dual Optimizer:
  [picture of graph]
  For my next experiment, I used Adadelta. With this optimizer, I got the same 3224697 trainable parameters, the same input set of 9549, at the last evaluation at step 2950, I got an accuracy of 0.732719 (73%), and a loss of 0.649049 with a train time of 34 minutes. This model performed with a bit higher accuracy of about 3% compared to RMSProp, about 1% better accuracy than SGD smalldata and about 30% better accuracy than the original larger dataset, but underperformed by around 7% than their smaller dataset model. This model trained in the same time as my SGD smalldata which is faster than the RMSProp by 2 minutes and is still slower than both their slower dataset by 32 minutes and their larger dataset model by 8 minutes. Loss is very low for this model, which mean the model performed much better than RMSProp. Loss, for the most part, decreased throughout training with a few spikes at the 184th step, and from steps 268 to 300 there was a lot of fluctuation from 0.8 to 0.4 loss, the rest of training, loss continued to steadily decrease fairly stably without any major fluctuations. Accuracy was fairly stable as well and increased without major dips throughout training. If I were to compare specific graphs from the paper, if I had a graph I believe that it would look similar to the graphs for the larger Tobacco-3482 dataset from the paper, A3: accuracy graph of Lightweight Text CNN:
  [picture of graph]
  My experiments did not perform as well as the original from the paper, but they did perform better than expected considering the problems with the data and memory allocation. If I had eval.py working, I could have gotten a better view of how well the data was fitted. I would like to make the prediction that the models are underfitted considering the such small amounts of data given, compared to the original model. Given a better machine with more computing power, perhaps I could have run the full dataset and have gotten better results, not just in accuracy, but in compressed time as well since that was the core purpose of optimizing the TextCNN.
